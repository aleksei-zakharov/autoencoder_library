{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether generated vol cubes are consistent with historical volatility cubes, we do the following procedure:\n",
    "\n",
    "1) Download the trained VAE model\n",
    "\n",
    "2) Take 1 volatility cube sample from test dataset\n",
    "\n",
    "3) Generate N volatility cubes\n",
    "\n",
    "4) We find one generated vol cube among N generated volatility cubes that is closest to the volatility cube from item 3. The closeness is measured by mean squared error between all data points in vol cube.\n",
    "\n",
    "5) We repeat items 1-4 for all vol cubes from test dataset and calculate the average MSE across all vol cubes from test dataset.\n",
    "\n",
    "We do steps 1-5 for random split and for temporal splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download, normalize and split vol cube data into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # to go to the main folder of the whole project\n",
    "\n",
    "# Download the data\n",
    "from src.data.vol.get_vol_cube_tenors_strikes_dates import get_vol_cube_tenors_strikes_dates\n",
    "data, opt_tenors, swap_tenors, strikes, dates = get_vol_cube_tenors_strikes_dates()\n",
    "\n",
    "# Normalize data\n",
    "from src.data.vol.normalizer import Normalizer\n",
    "normalizer = Normalizer()\n",
    "data_norm = normalizer.normalize(data)\n",
    "\n",
    "# Split train and test datasets\n",
    "dataset_split_type = 'random_split'\n",
    "from src.utils.get_train_test_datasets import get_train_test_datasets\n",
    "data_norm_train, dates_train, data_norm_test, dates_test = get_train_test_datasets(data_norm,\n",
    "                                                                                   dates,\n",
    "                                                                                   seed=0,\n",
    "                                                                                   train_ratio=0.8,\n",
    "                                                                                   type=dataset_split_type)\n",
    "data_train = normalizer.denormalize(data_norm_train)\n",
    "data_test = normalizer.denormalize(data_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model and its history from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alexi\\Documents\\All\\ETH_UZH\\MasterThesis\\_MT_Vol_cube\\code_my\\autoencoder_library\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexi\\Documents\\All\\ETH_UZH\\MasterThesis\\_MT_Vol_cube\\code_my\\autoencoder_library\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from src.utils.load_model_and_history import load_model_and_history\n",
    "\n",
    "NAME = 'vae_van_leaky_randomsplit_3_200_100_50_25_3000ep_bat16_1e-5'\n",
    "vae, history = load_model_and_history(NAME,\n",
    "                                      data_type='vol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether generated vol cubes are consistent with historical volatility cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse over all dates 1.35 bp, while max_err over all dates 40.32 bp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.visualization.vol.vol_cube_grids import vol_cube_grids\n",
    "strikes = ['ATM-100bp', 'ATM-50bp', 'ATM', 'ATM+50bp', 'ATM+100bp']  # graphs only for these strikes\n",
    "\n",
    "points_num = len(data_test[0].reshape(-1))\n",
    "\n",
    "# Generate N vol cubes\n",
    "N = 10_000\n",
    "latent_space_dim = vae.latent_space_dim\n",
    "mean = np.zeros(latent_space_dim)\n",
    "cov = np.eye(latent_space_dim)\n",
    "np.random.seed(0)\n",
    "z_initial = np.random.multivariate_normal(mean, cov, N)\n",
    "gen_vol_cubes = normalizer.denormalize(vae.decoder.predict(np.array(z_initial), verbose=0))\n",
    "\n",
    "# Find generated vol cube that fits our test dataset the best way in terms of mse error\n",
    "sq_errs_opt_all = 0\n",
    "max_err = 0\n",
    "\n",
    "for date_idx, date in enumerate(dates_test):\n",
    "    sq_err_opt = float('inf')\n",
    "    for i in range(N):\n",
    "        diff = data_test[date_idx] - gen_vol_cubes[i]\n",
    "        sq_err_curr = (diff**2).sum()\n",
    "        if sq_err_curr < sq_err_opt:\n",
    "            sq_err_opt = sq_err_curr\n",
    "            best_i = i\n",
    "                \n",
    "    sq_errs_opt_all += sq_err_opt\n",
    "\n",
    "    diff = data_test[date_idx] - gen_vol_cubes[best_i]\n",
    "    max_err_curr = abs(diff).max()\n",
    "    max_err = max(max_err_curr, max_err)\n",
    "\n",
    "print('mse over all dates', round((sq_errs_opt_all / len(dates_test) / points_num)**0.5, 2), \\\n",
    "      'bp, while max_err over all dates', round(max_err,2), 'bp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # to go to the main folder of the whole project\n",
    "\n",
    "from src.data.vol.get_vol_cube_tenors_strikes_dates import get_vol_cube_tenors_strikes_dates\n",
    "data, uniq_opt_tenors, uniq_swap_tenors, uniq_strikes, dates = get_vol_cube_tenors_strikes_dates()\n",
    "\n",
    "# Normalize data\n",
    "from src.data.vol.normalizer import Normalizer\n",
    "normalizer = Normalizer()\n",
    "data_norm = normalizer.normalize(data)\n",
    "\n",
    "# Split train and test datasets\n",
    "dataset_split_type = 'temporal_split'\n",
    "from src.utils.get_train_test_datasets import get_train_test_datasets\n",
    "data_norm_train, dates_train, data_norm_test, dates_test = get_train_test_datasets(data_norm,\n",
    "                                                                                   dates,\n",
    "                                                                                   type=dataset_split_type)\n",
    "\n",
    "data_train = normalizer.denormalize(data_norm_train)\n",
    "data_test = normalizer.denormalize(data_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model and its history from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexi\\Documents\\All\\ETH_UZH\\MasterThesis\\_MT_Vol_cube\\code_my\\autoencoder_library\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from src.utils.load_model_and_history import load_model_and_history\n",
    "\n",
    "NAME = 'vae_van_leaky_3_128_48_3000ep_bat16_3e-4'\n",
    "vae, history = load_model_and_history(NAME,\n",
    "                                      data_type='vol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether generated vol cubes are consistent with historical volatility cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse over all dates 5.36 bp, while max_err over all dates 63.11 bp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.visualization.vol.vol_cube_grids import vol_cube_grids\n",
    "strikes = ['ATM-100bp', 'ATM-50bp', 'ATM', 'ATM+50bp', 'ATM+100bp']  # graphs only for these strikes\n",
    "\n",
    "points_num = len(data_test[0].reshape(-1))\n",
    "\n",
    "# Generate N vol cubes\n",
    "N = 10_000\n",
    "latent_space_dim = vae.latent_space_dim\n",
    "mean = np.zeros(latent_space_dim)\n",
    "cov = np.eye(latent_space_dim)\n",
    "np.random.seed(0)\n",
    "z_initial = np.random.multivariate_normal(mean, cov, N)\n",
    "gen_vol_cubes = normalizer.denormalize(vae.decoder.predict(np.array(z_initial), verbose=0))\n",
    "\n",
    "# Find generated vol cube that fits our test dataset the best way in terms of mse error\n",
    "sq_errs_opt_all = 0\n",
    "max_err = 0\n",
    "\n",
    "for date_idx, date in enumerate(dates_test):\n",
    "    sq_err_opt = float('inf')\n",
    "    for i in range(N):\n",
    "        diff = data_test[date_idx] - gen_vol_cubes[i]\n",
    "        sq_err_curr = (diff**2).sum()\n",
    "        if sq_err_curr < sq_err_opt:\n",
    "            sq_err_opt = sq_err_curr\n",
    "            best_i = i\n",
    "                \n",
    "    sq_errs_opt_all += sq_err_opt\n",
    "\n",
    "    diff = data_test[date_idx] - gen_vol_cubes[best_i]\n",
    "    max_err_curr = abs(diff).max()\n",
    "    max_err = max(max_err_curr, max_err)\n",
    "\n",
    "print('mse over all dates', round((sq_errs_opt_all / len(dates_test) / points_num)**0.5, 2), \\\n",
    "      'bp, while max_err over all dates', round(max_err,2), 'bp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
