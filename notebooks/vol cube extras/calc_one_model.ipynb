{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download, normalize and split vol cube data into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # to go to the main folder of the whole project\n",
    "\n",
    "# Download the data\n",
    "from src.data.vol.get_vol_cube_tenors_strikes_dates import get_vol_cube_tenors_strikes_dates\n",
    "data, opt_tenors, swap_tenors, strikes, dates = get_vol_cube_tenors_strikes_dates()\n",
    "\n",
    "# Normalize data\n",
    "from src.data.vol.normalizer import Normalizer\n",
    "normalizer = Normalizer()\n",
    "data_norm = normalizer.normalize(data)\n",
    "\n",
    "# Split train and test datasets\n",
    "dataset_split_type = 'random_split'\n",
    "from src.utils.get_train_test_datasets import get_train_test_datasets\n",
    "data_norm_train, dates_train, data_norm_test, dates_test = get_train_test_datasets(data_norm,\n",
    "                                                                                   dates,\n",
    "                                                                                   seed=0,\n",
    "                                                                                   train_ratio=0.8,\n",
    "                                                                                   type=dataset_split_type)\n",
    "data_train = normalizer.denormalize(data_norm_train)\n",
    "data_test = normalizer.denormalize(data_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and train the model (it takes 10 minutes to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3000\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_LAYERS_NODES =  [200, 100, 50, 25]\n",
    "LOSS_TYPE = 'mse'\n",
    "BETA = '1e-5'   # weight of Kullback-Leibler (KL) loss when we calculate total loss as a sum of KL loss and reconstruction loss.\n",
    "LATENT_SPACE_DIM = 3\n",
    "ACTIVATION = 'leaky_relu'   # or 'relu'\n",
    "\n",
    "# Create the name of the model based on characteristics\n",
    "NAME = 'vae_van_' + ACTIVATION + '_' + dataset_split_type + '_' + str(LATENT_SPACE_DIM) + '_' \n",
    "for i, nodes in enumerate(HIDDEN_LAYERS_NODES):\n",
    "    NAME += str(nodes) + '_'\n",
    "NAME += str(EPOCHS) + 'ep_bat' + str(BATCH_SIZE) +'_' + BETA\n",
    "print(NAME)\n",
    "\n",
    "\n",
    "# Create VAE and fit it\n",
    "from src.models.vae_vanilla import VaeVanilla\n",
    "vae = VaeVanilla(input_shape=data_norm_train.shape[1:],\n",
    "                 hidden_layers_nodes=HIDDEN_LAYERS_NODES,\n",
    "                 latent_space_dim=LATENT_SPACE_DIM,\n",
    "                 loss_type=LOSS_TYPE,  \n",
    "                 beta=float(BETA),\n",
    "                 activation='leaky_relu')\n",
    "vae.compile(optimizer='adam')\n",
    "history = vae.fit(data_norm_train, \n",
    "                  epochs=EPOCHS, \n",
    "                  verbose=0,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  validation_data=data_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.save_model_and_history import save_model_and_history\n",
    "save_model_and_history(vae, \n",
    "                       history, \n",
    "                       NAME,\n",
    "                       data_type='vol')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
